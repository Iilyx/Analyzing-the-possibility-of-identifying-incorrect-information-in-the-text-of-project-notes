{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Финальный результат (версия для параграфов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем необходимые библиотеки\n",
    "\n",
    "from docx import Document\n",
    "from docx.shared import RGBColor\n",
    "from docx.enum.text import WD_COLOR_INDEX\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import statistics as stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка данных из файла\n",
    "\n",
    "def load_data_from_docx(file_name):\n",
    "    document = Document(file_name)\n",
    "    words_to_delete = []\n",
    "    for para in document.paragraphs:\n",
    "        if para.style.name.startswith('Heading') and para.text.strip():\n",
    "            words_to_delete.append(para.text)\n",
    "    words_to_delete.append('Таблица')\n",
    "    rez,found_start=[],False\n",
    "    for paragraph in document.paragraphs:\n",
    "        if words_to_delete[0] in paragraph.text:\n",
    "            found_start = True\n",
    "        elif found_start:\n",
    "            rez.append(paragraph.text)\n",
    "    return [i for i in rez if i.strip() and not any(word in i for word in words_to_delete)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_parts=load_data_from_docx(\"П-07.22-6.1-КР-ТЧ.docx\")\n",
    "#real_parts=load_data_from_docx(\"ПР_12_20_ПД_ПОС_Проект_организации_строительства.docx_review.docx\")\n",
    "#len(real_parts)\n",
    "#real_parts\n",
    "data = real_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# расстояние каждого предложения до других (значения + сортировка)\n",
    "\n",
    "def generate_seq_of_similarity_inc(data:list):\n",
    "    results,results_values=[],[]\n",
    "    embedder = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "    sentence_embeddings = embedder.encode(data, convert_to_tensor=True)\n",
    "    for i in range(len(data)):\n",
    "        SENTENCE_NUM=i\n",
    "        given_sentence_embedding = sentence_embeddings[SENTENCE_NUM]\n",
    "        cosine_similarities_bert = util.pytorch_cos_sim(given_sentence_embedding.reshape(1, -1), sentence_embeddings).numpy().flatten()\n",
    "        results_values.append(cosine_similarities_bert)\n",
    "        results.append(cosine_similarities_bert.argsort())\n",
    "    return results,results_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results,results_values=generate_seq_of_similarity_inc(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# поиск \"непохожих\" предложений\n",
    "\n",
    "def get_frequence(results,results_values,isPrint=False):\n",
    "    sorts=[]\n",
    "    for j in range(len(results)):\n",
    "        sorts.append([i for i in results[j] if results_values[j][i]<np.mean(results_values)])\n",
    "    flat_arr = [val for arr in sorts for val in arr]\n",
    "    freq_dict = {val: flat_arr.count(val) for val in set(flat_arr)}\n",
    "    sorted_freq_list=sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    global data\n",
    "    data_stat=[i[1] for i in sorted_freq_list]\n",
    "    find,mark=[],False\n",
    "    if isPrint:\n",
    "        print(\"\\nЧастота встречаемости элементов (статистический анализ):\")\n",
    "        mark=True\n",
    "    for val, count in sorted_freq_list: \n",
    "        if count >= round(stat.mean(data_stat)+1.9*stat.stdev(data_stat)):\n",
    "            if mark:\n",
    "                print(f\"{val}: {count}\")\n",
    "                print(data[val])\n",
    "            find.append(data[val])\n",
    "            \n",
    "    return sorted_freq_list,find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_freq_list,find=get_frequence(results,results_values,False)\n",
    "#sorted_freq_list\n",
    "find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выгрузка результатов в отдельный файл\n",
    "\n",
    "def get_review(file_name,find):\n",
    "    document = Document(file_name)\n",
    "    # Список предложений, которые нужно выделить\n",
    "    sentences_to_highlight = find\n",
    "    # Проходим по всем абзацам в документе\n",
    "    for paragraph in document.paragraphs:\n",
    "        # Проверяем, содержит ли абзац какое-либо предложение из списка\n",
    "        for sentence in sentences_to_highlight:\n",
    "            if sentence in paragraph.text:\n",
    "                # Если предложение найдено, выделяем его\n",
    "                for run in paragraph.runs:\n",
    "                    if run.text in sentence:\n",
    "                        #run.font.color.rgb = RGBColor(255, 0, 0)\n",
    "                        run.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "\n",
    "    return document.save(f'{file_name}_review.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_review(\"П-07.22-6.1-КР-ТЧ.docx\",find)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Все в одной функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем необходимые библиотеки\n",
    "\n",
    "from docx import Document\n",
    "from docx.shared import RGBColor\n",
    "from docx.enum.text import WD_COLOR_INDEX\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import statistics as stat\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "from razdel import sentenize, tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  тестовые версии функции load_data_from_docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузка данных из файла\n",
    "\n",
    "def load_data_from_docx(file_name):\n",
    "    document = Document(file_name)\n",
    "    words_to_delete = []\n",
    "    for para in document.paragraphs:\n",
    "        if para.style.name.startswith('Heading') and para.text.strip():\n",
    "            words_to_delete.append(para.text)\n",
    "    words_to_delete.append('Таблица')\n",
    "    rez,found_start=[],False\n",
    "    for paragraph in document.paragraphs:\n",
    "        if words_to_delete[0] in paragraph.text:\n",
    "            found_start = True\n",
    "        elif found_start:\n",
    "            rez.append(paragraph.text)\n",
    "    return [i for i in rez if i.strip() and not any(word in i for word in words_to_delete)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузка данных из файла\n",
    "\n",
    "def load_data_from_docx(file_name):\n",
    "    document = Document(file_name)\n",
    "    words_to_delete = []\n",
    "    for para in document.paragraphs:\n",
    "        if para.style.name.startswith('Heading') and para.text.strip():\n",
    "            words_to_delete.append(para.text)\n",
    "    bold_under_to_delete = []\n",
    "\n",
    "    #кусок по выюорке подзаголовков\n",
    "\n",
    "    for paragraph in document.paragraphs:\n",
    "         paragraph_text = \"\"\n",
    "         for run in paragraph.runs:\n",
    "             if run.bold and run.underline:\n",
    "                 paragraph_text += run.text\n",
    "         if paragraph_text:\n",
    "             bold_under_to_delete.append(paragraph_text)\n",
    "\n",
    "\n",
    "    bold_under_to_delete.append(\"Таблица\")\n",
    "\n",
    "    #проверка по регуляркам\n",
    "\n",
    "    def split_sentences(text):\n",
    "        sentences = re.split(r'(?<!г)\\.(?=\\s+[А-Я])(?=(?:[^«»]*«[^»]*»)*[^«»]*$)(?<!\\d\\.)', text)\n",
    "        return sentences\n",
    "    \n",
    "    rez,rez_heads,found_start=[],[],False\n",
    "    for paragraph in document.paragraphs:\n",
    "        if any(word in paragraph.text for word in words_to_delete):\n",
    "            found_start = True\n",
    "            if rez_heads:\n",
    "                rez.append(rez_heads)\n",
    "                rez_heads = []\n",
    "            \n",
    "            #добавление заголовка\n",
    "\n",
    "            para_sentences = split_sentences(paragraph.text)\n",
    "            for sentence in para_sentences:\n",
    "                sentence = sentence.strip()\n",
    "                if sentence and (sentence not in bold_under_to_delete and any(word not in sentence for word in bold_under_to_delete)):\n",
    "                    rez_heads.append(sentence)\n",
    "\n",
    "        elif found_start:\n",
    "            para_sentences = split_sentences(paragraph.text)\n",
    "            #para_sentences =paragraph.text.split('. ')\n",
    "            for sentence in para_sentences:\n",
    "                sentence = sentence.strip()\n",
    "                if sentence and (sentence not in bold_under_to_delete and any(word not in sentence for word in bold_under_to_delete)):\n",
    "                #if sentence and \"Таблица\" not in sentence:\n",
    "                    rez_heads.append(sentence)\n",
    "\n",
    "    if rez_heads:\n",
    "        rez.append(rez_heads)\n",
    "        \n",
    "    return rez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### итоговая функция load_data_from_docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка данных из файла\n",
    "\n",
    "def load_data_from_docx(file_name):\n",
    "    document = Document(file_name)\n",
    "    words_to_delete = []\n",
    "    for para in document.paragraphs:\n",
    "        if para.style.name.startswith('Heading') and para.text.strip():\n",
    "            words_to_delete.append(para.text)\n",
    "    bold_under_to_delete = []\n",
    "\n",
    "    # кусок по выборке подзаголовков\n",
    "\n",
    "    # for paragraph in document.paragraphs:\n",
    "    #     paragraph_text = \"\"\n",
    "    #     for run in paragraph.runs:\n",
    "    #         if run.bold and run.underline:\n",
    "    #             paragraph_text += run.text\n",
    "    #     if paragraph_text:\n",
    "    #         bold_under_to_delete.append(paragraph_text)\n",
    "\n",
    "    bold_under_to_delete.append(\"Таблица\")\n",
    "\n",
    "    # проверка по регуляркам\n",
    "    #def split_sentences(text):\n",
    "        #sentences = re.split(r'(?=(?:[^«»]*«[^»]*»)*[^«»]*$)', text)\n",
    "            #return sentences\n",
    "        #sentences = re.split(r'(?<!\\b(?:т\\.д|т\\.п|р\\.п))\\.(?=[А-Я])\\.(?<!г)\\.(?=[А-Яа-я])(?=(?:[^«»]*«[^»]*»)*[^«»]*$)(?<!\\d\\.)',text)\n",
    "\n",
    "    # def split_sentences(text):\n",
    "    #      sentences =list(sentenize(text))\n",
    "    #      return [i.text for i in sentences]\n",
    "\n",
    "    def split_sentences(text):\n",
    "        sentences = list(sentenize(text))\n",
    "        sentence_texts = [i.text for i in sentences]\n",
    "    \n",
    "        combined_sentences = []\n",
    "        temp_sentence = \"\"\n",
    "    \n",
    "        for i, sentence in enumerate(sentence_texts):\n",
    "            temp_sentence += sentence\n",
    "        \n",
    "            # Проверка наличия непарных кавычек в предложении\n",
    "            open_quotes = temp_sentence.count('\"') % 2\n",
    "        \n",
    "            if open_quotes == 0:\n",
    "                combined_sentences.append(temp_sentence.strip())\n",
    "                temp_sentence = \"\"\n",
    "            else:\n",
    "                temp_sentence += \" \"\n",
    "    \n",
    "        # Если в конце осталась незакрытая кавычка\n",
    "        if temp_sentence:\n",
    "            combined_sentences.append(temp_sentence.strip())\n",
    "    \n",
    "        return combined_sentences\n",
    "        \n",
    "        \n",
    "    # раскрытие заголовков\n",
    "    def add_data_from_paragraph(rez_heads):\n",
    "                para_sentences = split_sentences(paragraph.text)\n",
    "                for sentence in para_sentences:\n",
    "                    sentence = sentence.strip()\n",
    "                    if sentence and (sentence not in bold_under_to_delete and any(word not in sentence for word in bold_under_to_delete)):\n",
    "                        rez_heads.append(sentence)\n",
    "                return rez_heads\n",
    "    \n",
    "    rez,rez_heads,found_start=[],[],False\n",
    "    for paragraph in document.paragraphs:\n",
    "        if any(word in paragraph.text for word in words_to_delete):\n",
    "            found_start = True\n",
    "            if rez_heads:\n",
    "                rez.append(rez_heads)\n",
    "                rez_heads = []\n",
    "            \n",
    "            # добавление заголовка\n",
    "            rez_heads=add_data_from_paragraph(rez_heads)\n",
    "\n",
    "        elif found_start:\n",
    "            rez_heads=add_data_from_paragraph(rez_heads)\n",
    "    \n",
    "    if rez_heads:\n",
    "        rez.append(rez_heads)\n",
    "        \n",
    "    return rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_parts=load_data_from_docx(\"П-07.22-6.1-КР-ТЧ.docx\")\n",
    "data=real_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# расстояние каждого предложения до других (значения + сортировка)\n",
    "\n",
    "def generate_seq_of_similarity_inc(data:list):\n",
    "    results,results_values=[],[]\n",
    "    embedder = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "    sentence_embeddings = embedder.encode(data, convert_to_tensor=True)\n",
    "    for i in range(len(data)):\n",
    "        SENTENCE_NUM=i\n",
    "        given_sentence_embedding = sentence_embeddings[SENTENCE_NUM]\n",
    "        cosine_similarities_bert = util.pytorch_cos_sim(given_sentence_embedding.reshape(1, -1), sentence_embeddings).numpy().flatten()\n",
    "        results_values.append(cosine_similarities_bert)\n",
    "        #results.append(cosine_similarities_bert.argsort())\n",
    "    data_outliner=np.mean(results_values)+1.9*np.std(results_values)\n",
    "\n",
    "\n",
    "    for i, lst in enumerate(results_values):\n",
    "        lst = lst\n",
    "        mask = (lst > data_outliner) & (lst < 0.99)\n",
    "        lst = lst[~mask]  \n",
    "        results_values[i] = np.array(lst)\n",
    "        results.append(np.argsort(lst))\n",
    "   \n",
    "\n",
    "    # for j, (i, _) in enumerate(zip(results, results_values)):\n",
    "    #     for item in i:\n",
    "    #         if _[item] > data_outliner and _[item] < 0.99:\n",
    "    #             mask = np.isin(i, item)\n",
    "    #             i = np.array(i[~mask])\n",
    "    #             mask2=np.isin(_,_[item])\n",
    "    #             _ = np.array(_ [~mask2])\n",
    "    #     results[j] = i\n",
    "    #     results_values[j] = _\n",
    "    \n",
    "    return results,results_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[]\n",
    "for i in data:\n",
    "    res.append(generate_seq_of_similarity_inc(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequence(results, results_values, data, isPrint=False):\n",
    "    if len(results) == 1:\n",
    "        print(results)\n",
    "        sorts = [i for i in results[0] if results_values[0][i] < np.median(results_values[0])]\n",
    "        freq_dict = {val: sorts.count(val) for val in set(sorts)}\n",
    "        sorted_freq_list = sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        data_stat = [i[1] for i in sorted_freq_list]\n",
    "        if len(data_stat) == 0:\n",
    "            threshold = 0\n",
    "        else:\n",
    "            threshold = round(stat.mean(data_stat) + 1.9 * stat.stdev(data_stat))\n",
    "    else:\n",
    "        sorts = []\n",
    "        median = np.median(np.concatenate([np.array(inner) for inner in results_values]))\n",
    "        for j in range(len(results)):\n",
    "            sorts.append([i for i in results[j] if results_values[j][i] < median])\n",
    "        flat_arr = [val for arr in sorts for val in arr]\n",
    "        freq_dict = {val: flat_arr.count(val) for val in set(flat_arr)}\n",
    "        sorted_freq_list = sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        data_stat = [i[1] for i in sorted_freq_list]\n",
    "        threshold = round(stat.mean(data_stat) + 1.9 * stat.stdev(data_stat))\n",
    "\n",
    "    find, mark = [], False\n",
    "    if isPrint:\n",
    "        print(\"\\nЧастота встречаемости элементов (статистический анализ):\")\n",
    "        mark = True\n",
    "    for val, count in sorted_freq_list: \n",
    "        if count >= threshold:\n",
    "            if mark:\n",
    "                print(f\"{val}: {count}\")\n",
    "                print(data[val])\n",
    "            find.append(data[val])\n",
    "    print(sorted_freq_list)\n",
    "    print(threshold)\n",
    "    return find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find=[]\n",
    "for i in range(len(res)):\n",
    "    #print(len(res[i][0]),len(res[i][1]))\n",
    "    find.append(get_frequence(res[i][0],res[i][1],data[i],isPrint=True))\n",
    "\n",
    "find=[item for sublist in find for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import RGBColor\n",
    "from docx.enum.text import WD_COLOR_INDEX\n",
    "\n",
    "def get_review(file_name, find):\n",
    "    document = Document(file_name)\n",
    "    sentences_to_highlight = find\n",
    "\n",
    "    for paragraph in document.paragraphs:\n",
    "        for sentence in sentences_to_highlight:\n",
    "            if sentence in paragraph.text:\n",
    "                # Find the start and end indices of the sentence in the paragraph text\n",
    "\n",
    "                # Create a new run for the highlighted sentence\n",
    "                run = paragraph.add_run(sentence)\n",
    "                run.font.color.rgb = RGBColor(255, 0, 0)\n",
    "                run.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "\n",
    "                # Remove the original text from the paragraph\n",
    "                #paragraph.text = paragraph.text.replace(sentence, '')\n",
    "\n",
    "    document.save(f'{file_name}_review.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_review(\"П-07.22-6.1-КР-ТЧ.docx\",find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# итоговая функция\n",
    "\n",
    "def get_non_simillar_sentences(filename,see_outliners=False):\n",
    "    data = load_data_from_docx(filename)\n",
    "    res,find=[],[]\n",
    "    for i in data:\n",
    "        res.append(generate_seq_of_similarity_inc(i))\n",
    "        \n",
    "    for i in range(len(res)):\n",
    "        find.append(get_frequence(res[i][0],res[i][1],data[i],isPrint=True))\n",
    "\n",
    "    find=[item for sublist in find for item in sublist]\n",
    "    print(find)\n",
    "    get_review(filename,find)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_non_simillar_sentences(\"П-07.22-6.1-КР-ТЧ.docx\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Финальная результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем необходимые библиотеки\n",
    "\n",
    "from docx import Document\n",
    "from docx.shared import RGBColor\n",
    "from docx.enum.text import WD_COLOR_INDEX\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import statistics as stat\n",
    "from razdel import sentenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка данных из файла\n",
    "\n",
    "def load_data_from_docx(file_name,include_sub=True):\n",
    "    document = Document(file_name)\n",
    "    words_to_delete = []\n",
    "    for para in document.paragraphs:\n",
    "        if para.style.name.startswith('Heading') and para.text.strip():\n",
    "            words_to_delete.append(para.text)\n",
    "    bold_under_to_delete = []\n",
    "\n",
    "    # кусок по выборке подзаголовков\n",
    "    if not include_sub:\n",
    "        for paragraph in document.paragraphs:\n",
    "            paragraph_text = \"\"\n",
    "            for run in paragraph.runs:\n",
    "                if run.bold and run.underline:\n",
    "                    paragraph_text += run.text\n",
    "            if paragraph_text:\n",
    "                bold_under_to_delete.append(paragraph_text.strip())\n",
    "\n",
    "    bold_under_to_delete.append(\"Таблица\")\n",
    "\n",
    "    def split_sentences(text):\n",
    "        sentences = list(sentenize(text))\n",
    "        sentence_texts = [i.text for i in sentences]\n",
    "    \n",
    "        combined_sentences = []\n",
    "        temp_sentence = \"\"\n",
    "    \n",
    "        for i, sentence in enumerate(sentence_texts):\n",
    "            temp_sentence += sentence\n",
    "        \n",
    "            # Проверка наличия непарных кавычек в предложении\n",
    "            open_quotes = temp_sentence.count('\"') % 2\n",
    "        \n",
    "            if open_quotes == 0:\n",
    "                combined_sentences.append(temp_sentence.strip())\n",
    "                temp_sentence = \"\"\n",
    "            else:\n",
    "                temp_sentence += \" \"\n",
    "    \n",
    "        # Если в конце осталась незакрытая кавычка\n",
    "        if temp_sentence:\n",
    "            combined_sentences.append(temp_sentence.strip())\n",
    "    \n",
    "        return combined_sentences\n",
    "        \n",
    "        \n",
    "    # раскрытие заголовков\n",
    "    def add_data_from_paragraph(rez_heads):\n",
    "                para_sentences = split_sentences(paragraph.text)\n",
    "                for sentence in para_sentences:\n",
    "                    sentence = sentence.strip()\n",
    "                    if sentence and (sentence not in bold_under_to_delete and any(word not in sentence for word in bold_under_to_delete)):\n",
    "                        rez_heads.append(sentence)\n",
    "                return rez_heads\n",
    "    \n",
    "    rez,rez_heads,found_start=[],[],False\n",
    "    for paragraph in document.paragraphs:\n",
    "        if any(word in paragraph.text for word in words_to_delete):\n",
    "            found_start = True\n",
    "            if rez_heads:\n",
    "                rez.append(rez_heads)\n",
    "                rez_heads = []\n",
    "            \n",
    "            # добавление заголовка\n",
    "            rez_heads=add_data_from_paragraph(rez_heads)\n",
    "\n",
    "        elif found_start:\n",
    "            rez_heads=add_data_from_paragraph(rez_heads)\n",
    "    \n",
    "    if rez_heads:\n",
    "        rez.append(rez_heads)\n",
    "        \n",
    "    return rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# расстояние каждого предложения до других (значения + сортировка)\n",
    "\n",
    "def generate_seq_of_similarity_inc(data:list):\n",
    "    results,results_values=[],[]\n",
    "    embedder = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "    sentence_embeddings = embedder.encode(data, convert_to_tensor=True)\n",
    "    for i in range(len(data)):\n",
    "        SENTENCE_NUM=i\n",
    "        given_sentence_embedding = sentence_embeddings[SENTENCE_NUM]\n",
    "        cosine_similarities_bert = util.pytorch_cos_sim(given_sentence_embedding.reshape(1, -1), sentence_embeddings).numpy().flatten()\n",
    "        results_values.append(cosine_similarities_bert)\n",
    "\n",
    "    data_outliner=np.mean(results_values)+1.9*np.std(results_values)\n",
    "\n",
    "\n",
    "    for i, lst in enumerate(results_values):\n",
    "        lst = lst\n",
    "        mask = (lst > data_outliner) & (lst < 0.99)\n",
    "        lst = lst[~mask]  \n",
    "        results_values[i] = np.array(lst)\n",
    "        results.append(np.argsort(lst))\n",
    "    \n",
    "    return results,results_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequence(results, results_values, data, isPrint=False,coeff=1.9):\n",
    "    if len(results) == 1:\n",
    "        #print(results)\n",
    "        sorts = [i for i in results[0] if results_values[0][i] < np.median(results_values[0])]\n",
    "        freq_dict = {val: sorts.count(val) for val in set(sorts)}\n",
    "        sorted_freq_list = sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        data_stat = [i[1] for i in sorted_freq_list]\n",
    "        if len(data_stat) == 0:\n",
    "            threshold = 0\n",
    "        else:\n",
    "            threshold = round(stat.mean(data_stat) + coeff * stat.stdev(data_stat))\n",
    "    else:\n",
    "        sorts = []\n",
    "        median = np.median(np.concatenate([np.array(inner) for inner in results_values]))\n",
    "        for j in range(len(results)):\n",
    "            sorts.append([i for i in results[j] if results_values[j][i] < median])\n",
    "        flat_arr = [val for arr in sorts for val in arr]\n",
    "        freq_dict = {val: flat_arr.count(val) for val in set(flat_arr)}\n",
    "        sorted_freq_list = sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        data_stat = [i[1] for i in sorted_freq_list]\n",
    "        threshold = round(stat.mean(data_stat) + coeff * stat.stdev(data_stat))\n",
    "\n",
    "    find, mark = [], False\n",
    "    if isPrint:\n",
    "        print(\"\\nЧастота встречаемости элементов (статистический анализ):\")\n",
    "        mark = True\n",
    "    for val, count in sorted_freq_list: \n",
    "        if count >= threshold:\n",
    "            if mark:\n",
    "                print(f\"{val}: {count}\")\n",
    "                print(data[val])\n",
    "            find.append(data[val])\n",
    "    return find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review(file_name, find):\n",
    "    document = Document(file_name)\n",
    "    sentences_to_highlight = find\n",
    "\n",
    "    for paragraph in document.paragraphs:\n",
    "        for sentence in sentences_to_highlight:\n",
    "            if sentence in paragraph.text:\n",
    "            \n",
    "                run = paragraph.add_run(sentence)\n",
    "                run.font.color.rgb = RGBColor(255, 0, 0)\n",
    "                run.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "\n",
    "    document.save(f'{file_name}_review.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# итоговая функция\n",
    "\n",
    "def get_non_simillar_sentences(filename,see_outliners=False,include_sub=True,coeff=1.5):\n",
    "    data = load_data_from_docx(filename,include_sub)\n",
    "    res,find=[],[]\n",
    "    for i in data:\n",
    "        res.append(generate_seq_of_similarity_inc(i))\n",
    "        \n",
    "    for i in range(len(res)):\n",
    "        find.append(get_frequence(res[i][0],res[i][1],data[i],see_outliners,coeff))\n",
    "\n",
    "    find=np.unique([item for sublist in find for item in sublist])\n",
    "    get_review(filename,find)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_non_simillar_sentences(\"ПР_12_20_ПД_ПОС_Проект_организации_строительства.docx\",include_sub=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
